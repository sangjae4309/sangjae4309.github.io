<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <title>Sangjae Park</title>
  <meta name="description" content="Personal website of Sangjae Park.">
  <!-- Fonts and Icons -->
  <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons" />
  <!-- CSS Files -->
  <link rel="stylesheet" href="/assets/css/all.min.css">
  <link rel="stylesheet" href="/assets/css/academicons.min.css">
  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="stylesheet" href="/assets/css/sjpark.css">
  <link rel="canonical" href="/">
  <!-- Scripts -->
  <script async src="https://badge.dimensions.ai/badge.js" charset="utf-8"></script>
  <!-- Header -->
   <nav id="navbar" class="navbar blue darken-4 fixed-top navbar-expand-md z-depth-1 navbar-light">
    <div class="font-weight-normal  white-text">
      <a class="navbar-brand title font-weight-lighter white-text" href="/"><span class="font-weight-bold">Sangjae</span> Park</a>
      CS231n: Deep Learning for Computer Vision (Spring 2025)
    </div>
  </nav>
</head>
<body>
  <!-- Scrolling Progress Bar -->
  <progress id="progress" value="0">
    <div class="progress-container">
      <span class="progress-bar"></span>
    </div>
  </progress>
  
  
  <!-- Content -->
  <div class="content" style="padding: 15px 0px;">
    <!-- Introduction -->
    <div class="col-sm-auto pt-3 pb-3 abbr">
      This post presents a detailed walkthrough of a solution to
      <b>Standford CS231n:Convolutional Neural Networks for Visual Recognition</b> course (Spring 2025).
      This post is not only to provide working solution code, but to explain the underlying concepts, step by step,
      so you can build a deeper unerstanding of how and why the solution works.
      You can find the complete solution code on my github repo &rarr; <a id="link" href="https://github.com/sangjae4309/cs231n-solution">click this link</a>.
      <br><br>
      This post is just about the hard part of the assignment. And please keep in mind that some parts of solution may not be fully correct or understanable.
      If you spot any mistakes or areas for improvement, feel free to let me know. I'd truly appreciate it.
    </div>
    
    <!-- Index -->
    <div class="content " style="border-bottom: 1px solid #ddd; padding-bottom: 10px;">
      <h3 id="assignment1">Index</h3>
      <li class=>assignment1F
        <ul style="margin: 0;">
          <li><a href="#knn" id="link">Q1:k-Nearest Neighbor classifier</a></li>
          <li><a href="#softmax" id="link">Q2:Implement a Softmax Classifier</a></li>
          <li><a href="#two-layer" id="link">Q3:Two-Layer Neural Network</a></li>
          <li> Q4: Higher Level Representations: Image Features (__skip__)</li>
          <li> Q5: Training a fully connected network (__skip__)</li>
        </ul>
      </li>
      <li>assignment2
        <ul style="margin: 0;">
          <li><a href="#batch-norm" id="link">Q1: Batch Normalization</a></li>
          <li>Q2: Dropout (__skip__)</li>
          <li><a href="#cnn" id="link">Q3: Convolutional Neural Networks</a></li>
          <li>Q4: PyTorch on CIFAR-10 (__skip__)</li>
          <li>Q5: Image Captioning with Vanilla RNNs (__skip__)</li>
        </ul>
      </li>
      <li>assignment3
        <ul style="margin: 0;">
          <li><a href="#transformer" id="link">Q1:Image Captioning with Transformers</a></li>
          <li><a href="#self-supervised" id="link">Q2:Self-Supervised Learning for Image Classification</a></li>
          <li><a href="#diffusion" id="link">Q3:Denoising Diffusion Probabilistic Models</a></li>
          <li>Q4:CLIP and Dino (__skip__)</li>
        </ul>
      </li>
    </div>
    
    <!-- Assignment1 -->
    <div class="content ml-1 mr-1 p-0" style="border-bottom: 1px solid #ddd;">
      <h2>Assignment1</h2> 
      <!-- Q1:KNN -->
      <div class="col-sm-auto pt-3 pb-3 abbr">
        <h3 id="knn"> Q1:k-Nearest Neighbor classifier</h3>
        <h5>Implementing no-loop</h5>
        <img class="pt-1 pb-1" src="/assets/img/cs231n/knn-no-loop.png" style="width: 100%;">
        To approach the most challenging no-loop implementation, we start by expanding the quadratic equation used in the the K-Nearest Neighbors (KNN) algorithm.
        The self-squared terms can be simply computed using <code>np.sum()</code> and <code>np.square()</code>.
        For cross term, since it involves the sum of element-wise multiplications, it can be converted to a dot product.
        However, be careful to transpose one of the matrices to match alignment (see picture below).
      </div>
      <!-- Q2:Softmax -->
      <div class="col-sm-auto pt-3 pb-3 abbr">
        <h3 id="softmax"> Q2:Implement a Softmax Classifier</h3>
        <h5>Numeric stability</h5>
        <img class="pt-1 pb-1" src="/assets/img/cs231n/softmax-numeric-stability.png" style="width: 80%;"> <br>
        The exponentiating each score using the natural constant lead to large values and incur numeric instability.
        To address this issue, subtract the maximum score across all scores.
        This operation doesn't affect loss function because dividing both numerator and denominator by the same value leaves the result unchanged.
        Addtionally, should add a very small epsilon after subtracting to avoid taking logarithm of zero.
        <br><br>
        <h5>Computing gradient</h5>
        <img class="pt-1 pb-1" src="/assets/img/cs231n/softmax-gradient.png" style="width: 100%;">
        Take the derivatives with respect to the weights of the correct class label and the other class separately.
        For both weights, the gradient invloves multiplication of input data and the corresponding class probability.
        However, for the correct class label, the gradient gets subtracted by input data. Let's walk through a simple example, where we use softmax to train 1x3 input vector for 3-class classification task.
        <pre>
          <code class="language-python">
          X = [3.0, -2.6, 2.0]
          W = np.random([3,3])
          prb = [0.2, 0.6, 0.1]
          Y = [1,0,0]

          dW[0] = -X + X*0.2
          dW[1] = X*0.6
          dW[0] = X*0.1
          </code>
        </pre>
        <h5>Implementing no-loop gradient</h5>
        <img class="pt-1 pb-1" src="/assets/img/cs231n/softmax-gradient-vectorized.png" style="width: 80%;"><br>
        We can implement multiplication between input data and corresponding probability with simple dot product between input batch data and the probability matrix.
        This matrix contains the predicted probability scores for each class across all samples.
        To implement subtraction for correct class in the gradient, we need to slightly modify the probability matrix.
        Specifically, we subtract <code>1</code> at the position of corresponding to correct class bel for each sample.
        As shown in picture abov,e if the correct label for the first sample is class 2, then <code>-1</code> is added (not replaced) at the position in the matrix.
      </div>
      <!-- Q3:Two-layer -->
      <div class="col-sm-auto pt-3 pb-3 abbr">
        <h3 id="two-layer"> Q3:Two-Layer Neural Network</h3>
        <h5>Computing gradient</h5>
        The best way to understand fully-connected layer is writing down all computations manually.
        Let's walk through a simple example where 2x3 input vector and 3x2 weight vector.
        Assume <code>2x2 dout</code> comes from the upstream layer in the network.
        <img class="pt-1 pb-1" src="/assets/img/cs231n/two-layer-net.png" style="width: 100%;">        
        As seen above picture, the gradient of weight and input are another dot product and that of bias is simple summation along axis.
        <br>
        The remaining part mainly involves assembling the components and just setting up the training loop, which are relatively straightforward, so I'll skip over them here.
    </div>

    <!-- Assignment2 -->
    <div class="content ml-1 mr-1 p-0" style="border-bottom: 1px solid #ddd;">
      <h2>Assignment2</h2> 
      <!-- Q1:batch-norm -->
      <div class="col-sm-auto pt-3 pb-3 abbr">
        <h3 id="batch-norm"> Q1: Batch Normalization</h3>
        <h5>Getting gradient of gamma and beta </h5>
        <img class="pt-1 pb-1" src="/assets/img/cs231n/batchnorm-grad-gamma-beta.png" style="width: 100%;">
        The gradient beta and gamma are simply calculated by summing over the appropriate axis.
        You can refer to the illustration above using 2x3 input vector.
        <br><br>
        <h5>Getting gradient of input</h5>
        <img class="pt-1 pb-1" src="/assets/img/cs231n/batchnorm-grad-x.png" style="width: 100%;">
        We need to backpropagate through the entire computational graph.
        This invloves tracing the full path backward-from the final output, through the normalization step, and back to the original input-while applying the chain rule at each stage.
        Please take care to apply proper reshaping, as it is essential to align gradient correctly.
        For example, when tracing the path from the normalized input (shape [N,D]) to the standard deviation (shape [D,1]), you'll need to reduce the dimension by summing over the axis.
        <br><br>
      </div>
      <!-- Q3:CNN -->
      <div class="col-sm-auto pt-3 pb-3 abbr">
        <h3 id="cnn">Q3:Convolutional Neural Networks</h3>
        <h5>im2col (Image to Column) </h5>
        <img class="pt-1 pb-1" src="/assets/img/cs231n/cnn-im2col.png" style="width: 100%;">
        We will avoid using explicit loops to implement the sliding of the filter.
        Instead, we use the im2col tenchnique, which transforms the input vector into a matrix which each row corresponds to a receptive field aligned with the filter's sliding window.
        While this approach can lead to wastful memory footage-due to duplication-it can significantly accelerate computation by enabling efficient matrix multiplication.
        Refer to the illustration above, which shows a convolution operation on a 3x3 input with 2x2 filter and stride 1.
        <br><br>
        <h5>Getting gradient dW</h5>
        <img class="pt-1 pb-1" src="/assets/img/cs231n/cnn-dw.png" style="width: 100%;">
        The gradient with respect to W is computed by convolving the input vector with tge gradient received from the upstrea layer.
        The upstream gradient acts as the filter, and the stride is fixed to be 1.
        When it comes to original stride of CNN, that is reflected during the backpropagation by dilating the upstream gradient.
        Refering to stride-2 case in the illustration above will be helpful to understand.
        <br><br>
        <h5>Getting gradient dX</h5>
        <img class="pt-1 pb-1" src="/assets/img/cs231n/cnn-dx.png" style="width: 70%;"><br>
        The gradient with respect to input X is similiar excep that the weight acts as filter and dilation is applied to upstream gradient.
        Addtionally an extra one-pixel zero-padding is added around the upstream gradient.
        The illustration above demonstrates how to compute dX for a stride 2 case with 2x2 weight filter.
        Note that the resulting dX will include regions corresponding to the added zero-padding.
        These padded area are not part of the original input and must be explicitly trimmed to be the correct gradient.
      </div>
    </div>

    <!-- Assignment3 -->
    <div class="content ml-1 mr-1 p-0" style="border-bottom: 1px solid #ddd;">
      <h2>Assignment3</h2> 
      <!-- Q1:trans-former -->
      <div class="col-sm-auto pt-3 pb-3 abbr">
        <h3 id="transformer">Q1:Image Captioning with Transformers</h3>
        <h5>Implementing Multihed Self-Attention Layer </h5>
        <img class="pt-1 pb-1" src="/assets/img/cs231n/transformer-multihead-attention.png" style="width: 100%;">
        Multihead self-attention layer is composed of marix-multiplication, but with a few important distinctions.
        First, it is crucial to apply masking to prevent a token from looking ahead future tokens.
        For the illustartion above, a mask with the first row [1, inf, inf, inf] ensures that the first token can only refer to itself, not to any future tokens.
        Second, the target token is reference point for caculating attention scores.
        In the blue-highlighted box above, the first target token attends to the source tokens with weights [0.1,0.2,0.6,0.1], indicating that the third source token has the strongest influence.
        <br><br>
      </div>
      <!-- Q2:self-supervised -->
      <div class="col-sm-auto pt-3 pb-3 abbr">
        <h3 id="self-supervised">Q2:Self-Supervised Learning for Image Classification</h3>
        <h5>Implementing vectorized SimCLR loss</h5>
        Recall the loss of loss of positive pair \( i, j\) with batch \( N\):
        \[
          l(i,j) = -\log{\frac{exp(sim(z_i,z_j)/\tau)}{\sum_{k=1}^{2N}{1_{k\ne i} exp(sim(z_i,z_j)/\tau)}}}
        \]
        It's often easier to decouple the numerator and denominator when computing InfoNCE loss. 
        The numerator is straightforward—typically involving the cosine similarity between a positive pair—so we can safely skip detailing it here. 
        The denominator, however, requires more care: it involves computing similarities across all pairs and masking out.
        <img class="pt-1 pb-1" src="/assets/img/cs231n/self-supervised-simclr.png" style="width: 80%;"><br><br>
        The yellow box indicates one of positive pair.
        In vectorized form, cosine similarity can be efficiently computed using simple matrix multiplication via <code>compute_sim_matrix</code> function in assignment code.
        However, don't forget to mask-out in denominator for InfoNCE which exclude score of itself.
        <br><br>
      </div>
      <!-- Q2:self-supervised -->
      <div class="col-sm-auto pt-3 pb-3 abbr">
        <h3 id="diffusion">Q3:DDPM(Denoising Diffusion Probabilistic Models)</h3>
        <code>q_sample()</code>: The main novelty of paper is to skip forward pass and directly compute \( x_t \) from \( x_0 \).
        Just implement the function paper said as below.
        \[
          x_t=\sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon
        \]
        <code>p_loss()</code>: compute weighte MSE loss. Note that the \epsilon-target of what model should predicts-can be noise and x_start.
        \[
          L=\frac{1}{DHW}\sum{W|\epsilon-model(x_t,t)|^2}
        \]
        <code>p_sample()</code>: predict noise and directly compute x_start (i.e, original image) by manipulating the paper conclusion of forward pass (i.e, <code>q_sample()</code>).
        \[ \epsilon_0 = model(x_t, t)\]
        \[ x_0 = \frac{(x_t - \sqrt{1-\bar{\alpha}_t}\epsilon_0)}{\sqrt{\bar{\alpha}_t}} \]
        
      </div>
    </div>
  </div>

  <!-- Footer -->
  <footer>
    &copy; Copyright <span id="spanYear"></span> Sangjae Park.
  </footer>

  <!-- Core JavaScript Files -->
  <script src="/assets/js/jquery.min.js" type="text/javascript"></script>
  <script src="/assets/js/popper.min.js" type="text/javascript"></script>
  <script src="/assets/js/bootstrap.min.js" type="text/javascript"></script>
  <script src="/assets/js/mdb.min.js" type="text/javascript"></script>
  <script async="" src="https://cdnjs.cloudflare.com/ajax/libs/masonry/4.2.2/masonry.pkgd.min.js" integrity="sha384-GNFwBvfVxBkLMJpYMOABq3c+d3KnQxudP/mGPkzpZSTYykLBNsZEnG2D9G/X/+7D" crossorigin="anonymous"></script>
  <script src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"></script>
  <script src="/assets/js/common.js"></script>
  <script src="/assets/js/scrolling.js"></script>
  <!-- Code Syntax Highlighting -->
  <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet">
  <script src="/assets/js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</body>
</html>
